
from collections import Counter
from math import log2

# Function to calculate entropy
def entropy(y):
    counts = np.bincount(y)
    probabilities = counts / len(y)
    return -np.sum([p * log2(p) for p in probabilities if p > 0])

# Function to calculate information gain
def information_gain(X, y, feature):
    # Calculate the entropy of the entire dataset
    original_entropy = entropy(y)

    # Values and counts for the selected feature
    values, counts = np.unique(X[:, feature], return_counts=True)

    # Weighted entropy after the split
    weighted_entropy = np.sum([(counts[i] / np.sum(counts)) * entropy(y[X[:, feature] == v])
                               for i, v in enumerate(values)])

    # Information gain is the reduction in entropy
    return original_entropy - weighted_entropy

# ID3 Algorithm implementation
class ID3Tree:
    def __init__(self, max_depth=None):
        self.max_depth = max_depth
        self.tree = None

    # Function to fit the decision tree
    def fit(self, X, y, depth=0):
        # If all labels are the same, return the label
        if len(np.unique(y)) == 1:
            return np.unique(y)[0]

        # If no features left or maximum depth is reached, return the most common label
        if len(X[0]) == 0 or (self.max_depth is not None and depth >= self.max_depth):
            return Counter(y).most_common(1)[0][0]

        # Find the best feature to split on
        best_feature = np.argmax([information_gain(X, y, feature) for feature in range(X.shape[1])])

        # Create the tree as a dictionary
        tree = {best_feature: {}}

        # Recursively build the tree
        for value in np.unique(X[:, best_feature]):
            sub_X = X[X[:, best_feature] == value]
            sub_y = y[X[:, best_feature] == value]

            # Remove the used feature and call ID3 recursively
            subtree = self.fit(np.delete(sub_X, best_feature, axis=1), sub_y, depth+1)

            # Add subtree to the tree
            tree[best_feature][value] = subtree

        return tree

    # Train the tree
    def train(self, X, y):
        self.tree = self.fit(X, y)

    # Function to predict a single sample
    def predict_sample(self, tree, sample):
        if not isinstance(tree, dict):
            return tree
        feature = next(iter(tree))
        value = sample[feature]
        if value in tree[feature]:
            subtree = tree[feature][value]
            return self.predict_sample(subtree, np.delete(sample, feature))
        else:
            return Counter(y_train_1).most_common(1)[0][0]

    # Predict function for a batch of samples
    def predict(self, X):
        return np.array([self.predict_sample(self.tree, sample) for sample in X])

# Preparing data (Patient 1)
X_train_1_np = X_train_1.values
y_train_1_np = y_train_1.values
X_test_1_np = X_test_1.values
y_test_1_np = y_test_1.values

# Train the ID3 classifier
id3_tree = ID3Tree(max_depth=5)  # Set a maximum depth to avoid overfitting
id3_tree.train(X_train_1_np, y_train_1_np)

# Predict on test data
y_pred_1_custom = id3_tree.predict(X_test_1_np)

# Evaluate the model
accuracy_1_custom = accuracy_score(y_test_1_np, y_pred_1_custom)
print(f'Accuracy of custom ID3 on Patient 1: {accuracy_1_custom}')
